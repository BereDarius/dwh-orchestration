# YouTube to Databricks Pipeline Configuration (Staging)
pipeline:
  name: youtube_to_databricks
  description: "Ingest YouTube channel and video data into Databricks"
  environment: stage

  source:
    config_file: "sources/youtube_api.yaml"
    resources:
      - channel_stats
      - videos
    params:
      channel_id: "UCqj7Cz7revf5maW9g5pgNcg"
      max_videos: 500

  destination:
    config_file: "destinations/databricks.yaml"
    dataset_name: "youtube_analytics"

  schedule:
    enabled: true
    cron: "0 */4 * * *" # Every 4 hours in stage
    timezone: "UTC"
    catchup: false
    max_active_runs: 1

  execution:
    parallelism: 4
    timeout: 7200 # 2 hours
    retries: 3
    retry_delay: 600 # 10 minutes

    max_memory_mb: 4096
    max_cpu_cores: 4

  transformations:
    enabled: false

  data_quality:
    enabled: true
    fail_on_error: true # Fail on DQ issues in stage

    checks:
      - name: freshness_check
        type: freshness
        column: "_extracted_at"
        max_age_hours: 8

      - name: completeness_check
        type: completeness
        required_columns: ["id", "title", "published_at"]

      - name: volume_check
        type: volume
        min_rows: 1
        max_rows: 10000000

  monitoring:
    enabled: true
    metrics:
      - rows_processed
      - execution_time
      - error_count
      - data_quality_score

  alerts:
    enabled: true
    channels:
      - type: email
        recipients: ["data-engineering@example.com"]
    on_failure: true
    on_success: false

  tags:
    - youtube
    - social_media
    - analytics
    - stage

  owner: "data-engineering"
  sla_hours: 12
